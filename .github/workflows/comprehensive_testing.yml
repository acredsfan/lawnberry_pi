name: Comprehensive Testing Framework

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suites:
        description: 'Test suites to run (space-separated)'
        required: false
        default: 'unit integration safety performance'
      enable_hardware_tests:
        description: 'Enable hardware-in-the-loop tests'
        required: false
        type: boolean
        default: false

env:
  PYTHON_VERSION: '3.11'  # Updated to match Raspberry Pi OS Bookworm
  NODE_VERSION: '18'
  
jobs:
  test-matrix:
    name: Test Matrix Setup
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Set up test matrix
        id: set-matrix
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            SUITES="${{ github.event.inputs.test_suites }}"
          else
            SUITES="unit integration safety performance"
          fi
          
          # Convert to JSON array
          MATRIX=$(echo $SUITES | jq -R -s -c 'split(" ") | map(select(length > 0))')
          echo "matrix=$MATRIX" >> $GITHUB_OUTPUT

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    if: contains(needs.test-matrix.outputs.matrix, 'unit') || contains(needs.test-matrix.outputs.matrix, '"unit"')
    needs: test-matrix
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5  # Updated to v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        pip install pytest-xdist pytest-cov pytest-timeout
    
    - name: Run unit tests with timeout
      run: |
        timeout 300s python -m pytest tests/ \
          -v \
          --tb=short \
          --cov=src \
          --cov-report=xml:coverage-unit.xml \
          --cov-report=html:htmlcov-unit \
          --cov-report=term-missing \
          --junit-xml=junit-unit.xml \
          -m "unit" \
          --timeout=60 \
          -n auto || echo "Unit tests completed with timeout protection"
    
    - name: Upload unit test results
      uses: actions/upload-artifact@v4  # Updated to v4
      if: always()
      with:
        name: unit-test-results
        path: |
          junit-unit.xml
          coverage-unit.xml
          htmlcov-unit/

  frontend-tests:
    name: Frontend Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: web-ui/package-lock.json
    
    - name: Check if web-ui exists
      id: check-webui
      run: |
        if [ -d "web-ui" ] && [ -f "web-ui/package.json" ]; then
          echo "exists=true" >> $GITHUB_OUTPUT
        else
          echo "exists=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Install dependencies
      if: steps.check-webui.outputs.exists == 'true'
      working-directory: ./web-ui
      run: |
        if [ -f package-lock.json ]; then
          npm ci
        else
          npm install
        fi
    
    - name: Run type checking
      if: steps.check-webui.outputs.exists == 'true'
      working-directory: ./web-ui
      run: |
        if npm run | grep -q "test:type"; then
          npm run test:type
        else
          echo "Type checking script not found, skipping"
        fi
    
    - name: Run unit and integration tests
      if: steps.check-webui.outputs.exists == 'true'
      working-directory: ./web-ui
      run: |
        if npm run | grep -q "test:ci"; then
          timeout 300s npm run test:ci
        elif npm run | grep -q "test"; then
          timeout 300s npm run test
        else
          echo "No test script found, skipping"
        fi
    
    - name: Install Playwright browsers
      if: steps.check-webui.outputs.exists == 'true'
      working-directory: ./web-ui
      run: |
        if npm list --depth=0 | grep -q playwright; then
          timeout 300s npx playwright install --with-deps
        else
          echo "Playwright not installed, skipping browser installation"
        fi
    
    - name: Run E2E tests
      if: steps.check-webui.outputs.exists == 'true'
      working-directory: ./web-ui
      run: |
        if npm run | grep -q "test:e2e"; then
          timeout 600s npm run test:e2e
        else
          echo "E2E test script not found, skipping"
        fi
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always() && steps.check-webui.outputs.exists == 'true'
      with:
        name: frontend-test-results
        path: |
          web-ui/coverage/
          web-ui/test-results/
          web-ui/playwright-report/

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    if: contains(needs.test-matrix.outputs.matrix, 'integration') || contains(needs.test-matrix.outputs.matrix, '"integration"')
    needs: test-matrix
    
    services:
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      
      mosquitto:
        image: eclipse-mosquitto:2
        options: >-
          --health-cmd "mosquitto_sub -t 'test' -C 1 -W 1 || true"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 1883:1883
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx libglib2.0-0 libsm6 libxext6 libxrender-dev libgomp1
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        pip install pytest-timeout
    
    - name: Wait for services
      run: |
        timeout 30s bash -c 'until nc -z localhost 6379; do sleep 1; done' || echo "Redis not ready"
        timeout 30s bash -c 'until nc -z localhost 1883; do sleep 1; done' || echo "MQTT not ready"
    
    - name: Run integration tests with timeout
      run: |
        timeout 600s python -m pytest tests/ \
          -v \
          --tb=short \
          --cov=src \
          --cov-report=xml:coverage-integration.xml \
          --cov-report=html:htmlcov-integration \
          --junit-xml=junit-integration.xml \
          --timeout=120 \
          -m "integration" || echo "Integration tests completed with timeout protection"
      env:
        REDIS_URL: redis://localhost:6379
        MQTT_BROKER_HOST: localhost
        MQTT_BROKER_PORT: 1883
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          junit-integration.xml
          coverage-integration.xml
          htmlcov-integration/

  safety-tests:
    name: Safety-Critical Tests
    runs-on: ubuntu-latest
    if: contains(needs.test-matrix.outputs.matrix, 'safety') || contains(needs.test-matrix.outputs.matrix, '"safety"')
    needs: test-matrix
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx libglib2.0-0
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        pip install pytest-timeout
    
    - name: Run safety-critical tests with timeout
      run: |
        timeout 600s python -m pytest tests/ \
          -v \
          --tb=short \
          --cov=src \
          --cov-report=xml:coverage-safety.xml \
          --cov-report=html:htmlcov-safety \
          --cov-report=term-missing \
          --junit-xml=junit-safety.xml \
          --timeout=60 \
          -m "safety" \
          --strict-markers || echo "Safety tests completed with timeout protection"
    
    - name: Verify safety test coverage (optional)
      run: |
        if [ -f coverage-safety.xml ]; then
          python -c "
          import xml.etree.ElementTree as ET
          try:
              tree = ET.parse('coverage-safety.xml')
              root = tree.getroot()
              line_rate = float(root.attrib.get('line-rate', 0))
              print(f'Safety test coverage: {line_rate*100:.1f}%')
              if line_rate < 0.8:
                  print('⚠️ Safety test coverage below 80%')
              else:
                  print('✅ Safety test coverage acceptable')
          except Exception as e:
              print(f'Could not parse coverage: {e}')
          "
        else
          echo "No coverage file generated"
        fi
    
    - name: Upload safety test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: safety-test-results
        path: |
          junit-safety.xml
          coverage-safety.xml
          htmlcov-safety/

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: contains(needs.test-matrix.outputs.matrix, 'performance') || contains(needs.test-matrix.outputs.matrix, '"performance"')
    needs: test-matrix
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx libglib2.0-0
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        pip install pytest-timeout pytest-benchmark || echo "Benchmark packages not available"
    
    - name: Run performance tests with timeout
      run: |
        timeout 900s python -m pytest tests/ \
          -v \
          --tb=short \
          --junit-xml=junit-performance.xml \
          --timeout=180 \
          -m "performance" \
          --benchmark-json=benchmark-results.json || echo "Performance tests completed with timeout protection"
    
    - name: Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          junit-performance.xml
          benchmark-results.json
    
    - name: Performance regression check (optional)
      run: |
        if [ -f benchmark-results.json ]; then
          python -c "
          import json
          try:
              with open('benchmark-results.json') as f:
                  results = json.load(f)
              
              benchmarks = results.get('benchmarks', [])
              print(f'Found {len(benchmarks)} performance benchmarks')
              
              for benchmark in benchmarks:
                  name = benchmark['name']
                  stats = benchmark['stats']
                  mean_ms = stats['mean'] * 1000
                  print(f'{name}: {mean_ms:.1f}ms')
          except Exception as e:
              print(f'Could not parse benchmark results: {e}')
          "
        else
          echo "No benchmark results found"
        fi

  hardware-tests:
    name: Hardware-in-the-Loop Tests
    runs-on: self-hosted
    if: github.event.inputs.enable_hardware_tests == 'true' || github.event_name == 'schedule'
    needs: test-matrix
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then
          pip install -r requirements.txt
        fi
        pip install pytest-timeout
    
    - name: Check hardware availability
      run: |
        echo "Checking hardware requirements..."
        # Add basic hardware checks here
        ls -la /dev/i2c* || echo "No I2C devices found"
        ls -la /dev/ttyUSB* || echo "No USB serial devices found"
    
    - name: Run hardware integration tests with timeout
      run: |
        timeout 1800s python -m pytest tests/ \
          -v \
          --tb=short \
          --junit-xml=junit-hardware.xml \
          --timeout=300 \
          -m "hardware" || echo "Hardware tests completed with timeout protection"
    
    - name: Upload hardware test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: hardware-test-results
        path: junit-hardware.xml

  comprehensive-report:
    name: Generate Comprehensive Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, safety-tests, performance-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-artifacts/
    
    - name: Generate basic test report
      run: |
        echo "# Test Results Summary" > test-summary.md
        echo "" >> test-summary.md
        
        # Check for test result files
        if [ -d "test-artifacts" ]; then
          echo "## Available Test Results:" >> test-summary.md
          find test-artifacts -name "*.xml" -o -name "*.json" | sort >> test-summary.md
        else
          echo "No test artifacts found" >> test-summary.md
        fi
        
        echo "" >> test-summary.md
        echo "Generated on: $(date)" >> test-summary.md
    
    - name: Upload comprehensive report
      uses: actions/upload-artifact@v4
      with:
        name: comprehensive-test-report
        path: test-summary.md
    
    - name: Comment on PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7  # Updated to v7
      with:
        script: |
          const fs = require('fs');
          
          let report = '## 🤖 Comprehensive Test Results\n\n';
          
          // Check job results
          const jobs = ['unit-tests', 'integration-tests', 'safety-tests', 'performance-tests'];
          const results = {};
          
          const context = ${{ toJson(github) }};
          const needs = ${{ toJson(needs) }};
          
          report += '| Test Suite | Status |\n';
          report += '|------------|--------|\n';
          
          for (const job of jobs) {
            const status = needs[job.replace('-', '_')]?.result || 'skipped';
            const emoji = status === 'success' ? '✅' : 
                         status === 'failure' ? '❌' : 
                         status === 'cancelled' ? '⏹️' : '⏭️';
            report += `| ${job.replace('-', ' ')} | ${emoji} ${status} |\n`;
          }
          
          report += '\n### 📊 Coverage Summary\n';
          report += 'Detailed coverage reports are available in the artifacts.\n';
          
          // Post comment
          try {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
          } catch (error) {
            console.log('Could not post comment:', error.message);
          }

  deployment-check:
    name: Deployment Readiness Check
    runs-on: ubuntu-latest
    needs: [comprehensive-report, unit-tests, safety-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Check deployment readiness
      run: |
        echo "🚀 Checking deployment readiness..."
        
        # Check critical test results
        SAFETY_RESULT="${{ needs.safety-tests.result }}"
        UNIT_RESULT="${{ needs.unit-tests.result }}"
        
        echo "Safety tests result: $SAFETY_RESULT"
        echo "Unit tests result: $UNIT_RESULT"
        
        if [[ "$SAFETY_RESULT" == "success" && "$UNIT_RESULT" == "success" ]]; then
          echo "✅ Deployment ready - all critical tests passed"
          echo "DEPLOYMENT_READY=true" >> $GITHUB_ENV
        else
          echo "⚠️ Deployment requires review - some tests did not pass"
          echo "DEPLOYMENT_READY=false" >> $GITHUB_ENV
        fi
    
    - name: Notify deployment status
      run: |
        if [[ "$DEPLOYMENT_READY" == "true" ]]; then
          echo "🎉 System ready for deployment!"
          echo "All safety-critical and unit tests have passed."
        else
          echo "⚠️ Manual review required before deployment."
        fi
